{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ddd0156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 원-핫 인코딩 결과:\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "words = np.array([\"자연어\", \"처리\", \"딥러닝\", \"머신러닝\"]).reshape(-1, 1)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot = encoder.fit_transform(words)\n",
    "\n",
    "print(\"✅ 원-핫 인코딩 결과:\")\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ae72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:07:33,916 : INFO : collecting all words and their counts\n",
      "2025-05-23 14:07:36,027 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-23 14:07:40,397 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2025-05-23 14:07:40,397 : INFO : Creating a fresh vocabulary\n",
      "2025-05-23 14:07:41,503 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 253854 unique words (100.00% of original 253854, drops 0)', 'datetime': '2025-05-23T14:07:41.503080', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:07:41,503 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 17005207 word corpus (100.00% of original 17005207, drops 0)', 'datetime': '2025-05-23T14:07:41.503080', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:07:43,227 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2025-05-23 14:07:43,239 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2025-05-23 14:07:43,239 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12819131.785650097 word corpus (75.4%% of prior 17005207)', 'datetime': '2025-05-23T14:07:43.239133', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:07:45,901 : INFO : estimated required memory for 253854 words and 100 dimensions: 330010200 bytes\n",
      "2025-05-23 14:07:45,901 : INFO : resetting layer weights\n",
      "2025-05-23 14:07:46,033 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-23T14:07:46.033100', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-05-23 14:07:46,033 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 253854 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-05-23T14:07:46.033100', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:07:48,432 : INFO : EPOCH 0 - PROGRESS: at 0.06% examples, 3202 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:49,432 : INFO : EPOCH 0 - PROGRESS: at 7.00% examples, 264992 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:50,430 : INFO : EPOCH 0 - PROGRESS: at 15.70% examples, 459083 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:51,437 : INFO : EPOCH 0 - PROGRESS: at 25.10% examples, 598158 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:52,437 : INFO : EPOCH 0 - PROGRESS: at 34.16% examples, 688800 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:07:53,439 : INFO : EPOCH 0 - PROGRESS: at 43.33% examples, 755191 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:54,439 : INFO : EPOCH 0 - PROGRESS: at 53.38% examples, 819665 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:55,446 : INFO : EPOCH 0 - PROGRESS: at 63.37% examples, 869154 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:56,441 : INFO : EPOCH 0 - PROGRESS: at 73.37% examples, 908780 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:57,463 : INFO : EPOCH 0 - PROGRESS: at 83.36% examples, 938880 words/s, in_qsize 5, out_qsize 1\n",
      "2025-05-23 14:07:58,463 : INFO : EPOCH 0 - PROGRESS: at 93.65% examples, 969149 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:07:59,272 : INFO : EPOCH 0: training on 17005207 raw words (12820203 effective words) took 13.2s, 971054 effective words/s\n",
      "2025-05-23 14:08:00,664 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 5399 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:01,684 : INFO : EPOCH 1 - PROGRESS: at 8.64% examples, 457175 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:02,688 : INFO : EPOCH 1 - PROGRESS: at 18.81% examples, 704513 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:03,692 : INFO : EPOCH 1 - PROGRESS: at 28.92% examples, 839285 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:04,692 : INFO : EPOCH 1 - PROGRESS: at 39.15% examples, 928408 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:05,681 : INFO : EPOCH 1 - PROGRESS: at 49.15% examples, 984300 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:06,695 : INFO : EPOCH 1 - PROGRESS: at 59.14% examples, 1024954 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:07,695 : INFO : EPOCH 1 - PROGRESS: at 69.25% examples, 1057212 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:08,692 : INFO : EPOCH 1 - PROGRESS: at 79.07% examples, 1076678 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:09,699 : INFO : EPOCH 1 - PROGRESS: at 88.71% examples, 1091882 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:10,703 : INFO : EPOCH 1 - PROGRESS: at 99.35% examples, 1114724 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:10,909 : INFO : EPOCH 1: training on 17005207 raw words (12819525 effective words) took 11.6s, 1101999 effective words/s\n",
      "2025-05-23 14:08:12,099 : INFO : EPOCH 2 - PROGRESS: at 0.06% examples, 6606 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:13,099 : INFO : EPOCH 2 - PROGRESS: at 9.76% examples, 567024 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:14,100 : INFO : EPOCH 2 - PROGRESS: at 19.99% examples, 798940 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:15,103 : INFO : EPOCH 2 - PROGRESS: at 30.69% examples, 937900 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:16,099 : INFO : EPOCH 2 - PROGRESS: at 41.50% examples, 1025079 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:17,106 : INFO : EPOCH 2 - PROGRESS: at 51.85% examples, 1074471 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:18,103 : INFO : EPOCH 2 - PROGRESS: at 62.02% examples, 1107184 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:19,116 : INFO : EPOCH 2 - PROGRESS: at 71.90% examples, 1125394 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:20,129 : INFO : EPOCH 2 - PROGRESS: at 81.78% examples, 1137917 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:21,131 : INFO : EPOCH 2 - PROGRESS: at 91.06% examples, 1142732 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:22,338 : INFO : EPOCH 2 - PROGRESS: at 99.47% examples, 1115917 words/s, in_qsize 4, out_qsize 1\n",
      "2025-05-23 14:08:22,400 : INFO : EPOCH 2: training on 17005207 raw words (12817783 effective words) took 11.5s, 1115572 effective words/s\n",
      "2025-05-23 14:08:23,780 : INFO : EPOCH 3 - PROGRESS: at 0.06% examples, 5724 words/s, in_qsize 5, out_qsize 2\n",
      "2025-05-23 14:08:24,782 : INFO : EPOCH 3 - PROGRESS: at 8.70% examples, 466209 words/s, in_qsize 6, out_qsize 1\n",
      "2025-05-23 14:08:25,790 : INFO : EPOCH 3 - PROGRESS: at 17.99% examples, 677698 words/s, in_qsize 5, out_qsize 1\n",
      "2025-05-23 14:08:26,793 : INFO : EPOCH 3 - PROGRESS: at 27.04% examples, 788211 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:27,791 : INFO : EPOCH 3 - PROGRESS: at 37.51% examples, 893263 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:28,800 : INFO : EPOCH 3 - PROGRESS: at 47.74% examples, 958937 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:29,798 : INFO : EPOCH 3 - PROGRESS: at 57.32% examples, 995476 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:30,812 : INFO : EPOCH 3 - PROGRESS: at 66.26% examples, 1012892 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:31,814 : INFO : EPOCH 3 - PROGRESS: at 74.72% examples, 1020614 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:32,813 : INFO : EPOCH 3 - PROGRESS: at 84.01% examples, 1035223 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:33,821 : INFO : EPOCH 3 - PROGRESS: at 93.83% examples, 1053979 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:34,637 : INFO : EPOCH 3: training on 17005207 raw words (12819637 effective words) took 12.2s, 1047720 effective words/s\n",
      "2025-05-23 14:08:36,415 : INFO : EPOCH 4 - PROGRESS: at 0.06% examples, 4430 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:37,413 : INFO : EPOCH 4 - PROGRESS: at 7.70% examples, 353741 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:38,418 : INFO : EPOCH 4 - PROGRESS: at 17.28% examples, 583893 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:39,423 : INFO : EPOCH 4 - PROGRESS: at 28.57% examples, 765254 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:40,423 : INFO : EPOCH 4 - PROGRESS: at 39.45% examples, 876054 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:41,423 : INFO : EPOCH 4 - PROGRESS: at 50.32% examples, 952520 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:42,430 : INFO : EPOCH 4 - PROGRESS: at 61.14% examples, 1008654 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:43,430 : INFO : EPOCH 4 - PROGRESS: at 71.78% examples, 1048959 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:44,431 : INFO : EPOCH 4 - PROGRESS: at 83.01% examples, 1086743 words/s, in_qsize 5, out_qsize 0\n",
      "2025-05-23 14:08:45,431 : INFO : EPOCH 4 - PROGRESS: at 94.12% examples, 1117489 words/s, in_qsize 6, out_qsize 0\n",
      "2025-05-23 14:08:46,130 : INFO : EPOCH 4: training on 17005207 raw words (12817375 effective words) took 11.5s, 1115802 effective words/s\n",
      "2025-05-23 14:08:46,131 : INFO : Word2Vec lifecycle event {'msg': 'training on 85026035 raw words (64094523 effective words) took 60.1s, 1066612 effective words/s', 'datetime': '2025-05-23T14:08:46.131195', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:08:46,131 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=253854, vector_size=100, alpha=0.025>', 'datetime': '2025-05-23T14:08:46.131195', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-23 14:08:46,132 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-23T14:08:46.132869', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-05-23 14:08:46,132 : INFO : storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "2025-05-23 14:08:46,303 : INFO : storing np array 'syn1neg' to word2vec.model.syn1neg.npy\n",
      "2025-05-23 14:08:46,474 : INFO : not storing attribute cum_table\n",
      "2025-05-23 14:08:46,614 : INFO : saved word2vec.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word2Vec 모델이 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 학습용 텍스트 파일 직접 지정\n",
    "input_file = 'text8.txt'  # 학습에 사용할 텍스트 파일 경로\n",
    "output_model = 'word2vec.model'  # 저장할 모델 경로\n",
    "\n",
    "sentences = word2vec.LineSentence(input_file)\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          vector_size=100,  # size -> vector_size\n",
    "                          min_count=1,\n",
    "                          window=10\n",
    "                          )\n",
    "\n",
    "model.save(output_model)\n",
    "print(\"✅ Word2Vec 모델이 성공적으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef176845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:08:46,672 : INFO : loading Word2Vec object from word2vec.model\n",
      "2025-05-23 14:08:46,748 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2025-05-23 14:08:46,758 : INFO : loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "2025-05-23 14:08:46,813 : INFO : loading syn1neg from word2vec.model.syn1neg.npy with mmap=None\n",
      "2025-05-23 14:08:46,864 : INFO : setting ignored attribute cum_table to None\n",
      "2025-05-23 14:08:48,670 : INFO : Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2025-05-23T14:08:48.670369', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen \t 0.7008485198020935\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 파일 불러오기\n",
    "model = word2vec.Word2Vec.load('word2vec.model')  # 모델 파일 로드\n",
    "\n",
    "# 유사한 단어 찾기\n",
    "results = model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(result[0], '\\t', result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5fd90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st result: ['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', '욕/Noun', 'ㅋㅋㅋ/KoreanParticle']\n",
      "2st result: ['것', '되다', '욕']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  # Twitter 대신 Okt 사용\n",
    "tagger = Okt()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "sentences = tokenize(u'이것도 되나욕 ㅋㅋㅋ')\n",
    "print('1st result:', sentences)\n",
    "\n",
    "noun_adv_verb_only_list = [word.split(\"/\")[0] for word in sentences if word.split(\"/\")[1] == \"Verb\" or word.split(\"/\")[1] == \"Adjective\"\n",
    "                          or word.split(\"/\")[1] == \"Noun\"]\n",
    "print('2st result:', noun_adv_verb_only_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd4f9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 150000/150000 [09:54<00:00, 252.12it/s]\n",
      "Training Word2Vec:   0%|          | 0/150000 [00:00<?, ?it/s]2025-05-23 14:20:59,280 : INFO : collecting all words and their counts\n",
      "2025-05-23 14:20:59,280 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-23 14:20:59,320 : INFO : PROGRESS: at sentence #10000, processed 146795 words, keeping 13496 word types\n",
      "2025-05-23 14:20:59,372 : INFO : PROGRESS: at sentence #20000, processed 289797 words, keeping 19101 word types\n",
      "Training Word2Vec:  14%|█▍        | 21058/150000 [00:00<00:00, 206829.53it/s]2025-05-23 14:20:59,411 : INFO : PROGRESS: at sentence #30000, processed 435241 words, keeping 23273 word types\n",
      "2025-05-23 14:20:59,447 : INFO : PROGRESS: at sentence #40000, processed 582173 words, keeping 26843 word types\n",
      "Training Word2Vec:  33%|███▎      | 49276/150000 [00:00<00:00, 249667.97it/s]2025-05-23 14:20:59,481 : INFO : PROGRESS: at sentence #50000, processed 725793 words, keeping 29853 word types\n",
      "2025-05-23 14:20:59,516 : INFO : PROGRESS: at sentence #60000, processed 869123 words, keeping 32425 word types\n",
      "2025-05-23 14:20:59,545 : INFO : PROGRESS: at sentence #70000, processed 1011062 words, keeping 34879 word types\n",
      "Training Word2Vec:  52%|█████▏    | 77416/150000 [00:00<00:00, 263446.95it/s]2025-05-23 14:20:59,595 : INFO : PROGRESS: at sentence #80000, processed 1154793 words, keeping 37108 word types\n",
      "2025-05-23 14:20:59,641 : INFO : PROGRESS: at sentence #90000, processed 1299814 words, keeping 39305 word types\n",
      "2025-05-23 14:20:59,665 : INFO : PROGRESS: at sentence #100000, processed 1442096 words, keeping 41262 word types\n",
      "Training Word2Vec:  71%|███████   | 106002/150000 [00:00<00:00, 264733.60it/s]2025-05-23 14:20:59,696 : INFO : PROGRESS: at sentence #110000, processed 1584545 words, keeping 43165 word types\n",
      "2025-05-23 14:20:59,741 : INFO : PROGRESS: at sentence #120000, processed 1729450 words, keeping 45000 word types\n",
      "2025-05-23 14:20:59,759 : INFO : PROGRESS: at sentence #130000, processed 1872946 words, keeping 46648 word types\n",
      "Training Word2Vec:  91%|█████████▏| 136929/150000 [00:00<00:00, 277443.02it/s]2025-05-23 14:20:59,799 : INFO : PROGRESS: at sentence #140000, processed 2016541 words, keeping 48363 word types\n",
      "Training Word2Vec: 100%|██████████| 150000/150000 [00:00<00:00, 269908.25it/s]\n",
      "2025-05-23 14:20:59,834 : INFO : collected 49895 word types from a corpus of 2159921 raw words and 150000 sentences\n",
      "2025-05-23 14:20:59,834 : INFO : Creating a fresh vocabulary\n",
      "2025-05-23 14:20:59,888 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 15409 unique words (30.88% of original 49895, drops 34486)', 'datetime': '2025-05-23T14:20:59.888138', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:20:59,892 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2105472 word corpus (97.48% of original 2159921, drops 54449)', 'datetime': '2025-05-23T14:20:59.892054', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:20:59,959 : INFO : deleting the raw counts dictionary of 49895 items\n",
      "2025-05-23 14:20:59,959 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-05-23 14:20:59,959 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1704347.8986687385 word corpus (80.9%% of prior 2105472)', 'datetime': '2025-05-23T14:20:59.959726', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:21:00,059 : INFO : estimated required memory for 15409 words and 100 dimensions: 20031700 bytes\n",
      "2025-05-23 14:21:00,059 : INFO : resetting layer weights\n",
      "2025-05-23 14:21:00,073 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-23T14:21:00.073662', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-05-23 14:21:00,073 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15409 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-05-23T14:21:00.073662', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:21:00,980 : INFO : EPOCH 0: training on 2159921 raw words (1704544 effective words) took 0.9s, 1881045 effective words/s\n",
      "2025-05-23 14:21:01,924 : INFO : EPOCH 1: training on 2159921 raw words (1704139 effective words) took 0.9s, 1840988 effective words/s\n",
      "2025-05-23 14:21:02,926 : INFO : EPOCH 2: training on 2159921 raw words (1704446 effective words) took 1.0s, 1717380 effective words/s\n",
      "2025-05-23 14:21:03,725 : INFO : EPOCH 3: training on 2159921 raw words (1705048 effective words) took 0.8s, 2124683 effective words/s\n",
      "2025-05-23 14:21:04,527 : INFO : EPOCH 4: training on 2159921 raw words (1704340 effective words) took 0.8s, 2166559 effective words/s\n",
      "2025-05-23 14:21:04,527 : INFO : Word2Vec lifecycle event {'msg': 'training on 10799605 raw words (8522517 effective words) took 4.5s, 1914716 effective words/s', 'datetime': '2025-05-23T14:21:04.527049', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:21:04,527 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15409, vector_size=100, alpha=0.025>', 'datetime': '2025-05-23T14:21:04.527049', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-23 14:21:04,527 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_trained.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-23T14:21:04.527049', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-05-23 14:21:04,527 : INFO : not storing attribute cum_table\n",
      "2025-05-23 14:21:04,543 : INFO : saved word2vec_trained.model\n",
      "2025-05-23 14:21:04,543 : INFO : loading Word2Vec object from word2vec_trained.model\n",
      "2025-05-23 14:21:04,578 : INFO : loading wv recursively from word2vec_trained.model.wv.* with mmap=None\n",
      "2025-05-23 14:21:04,578 : INFO : setting ignored attribute cum_table to None\n",
      "2025-05-23 14:21:04,680 : INFO : Word2Vec lifecycle event {'fname': 'word2vec_trained.model', 'datetime': '2025-05-23T14:21:04.680682', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('여자/Noun', 0.8186426758766174)]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "import codecs\n",
    "\n",
    "# 데이터 읽기\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]  # header 제외\n",
    "    return data\n",
    "\n",
    "ratings_train = read_data('ratings_train.txt')\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokens(doc):\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "# 리뷰 텍스트만 추출\n",
    "docs = [row[1] for row in ratings_train]\n",
    "\n",
    "# 토큰화 + tqdm\n",
    "data = [tokens(d) for d in tqdm(docs, desc=\"Tokenizing\")]\n",
    "\n",
    "# Word2Vec 학습 + tqdm\n",
    "w2v_model = Word2Vec(tqdm(data, desc=\"Training Word2Vec\"), vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "w2v_model.save('word2vec_trained.model')\n",
    "\n",
    "# 모델 다시 불러오기\n",
    "model = Word2Vec.load('word2vec_trained.model')\n",
    "\n",
    "# 가장 유사한 단어 출력\n",
    "print(w2v_model.wv.most_similar(positive=tokens(u'남자 여배우'), negative=tokens(u'배우'), topn=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0633fce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('전도연/Noun', 0.901394248008728), ('임수정/Noun', 0.9000663161277771), ('패닝/Noun', 0.8982387185096741), ('엄태웅/Noun', 0.8831029534339905), ('박용우/Noun', 0.8796902894973755), ('문근영/Noun', 0.879190981388092), ('고현정/Noun', 0.8754245638847351), ('하비에르/Noun', 0.8725006580352783), ('김민준/Noun', 0.8712255954742432), ('테론/Noun', 0.8706749081611633)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=tokenize(u'정우성 조인성'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ca563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    \n",
    "    tokens = [w.split('/')[0] for w in tagger.pos(sentence, norm=True, stem=True) if w.split('/')[1] in ['Noun', 'Adjective', 'Verb']]\n",
    "\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d561f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing_second: 100%|██████████| 5/5 [00:00<00:00, 427.70it/s]\n",
      "Training Word2Vec_second:   0%|          | 0/5 [00:00<?, ?it/s]2025-05-23 14:26:38,452 : INFO : collecting all words and their counts\n",
      "2025-05-23 14:26:38,453 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "Training Word2Vec_second: 100%|██████████| 5/5 [00:00<00:00, 1609.97it/s]\n",
      "2025-05-23 14:26:38,456 : INFO : collected 40 word types from a corpus of 53 raw words and 5 sentences\n",
      "2025-05-23 14:26:38,458 : INFO : Creating a fresh vocabulary\n",
      "2025-05-23 14:26:38,460 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 40 unique words (100.00% of original 40, drops 0)', 'datetime': '2025-05-23T14:26:38.460114', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:26:38,460 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 53 word corpus (100.00% of original 53, drops 0)', 'datetime': '2025-05-23T14:26:38.460114', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:26:38,461 : INFO : deleting the raw counts dictionary of 40 items\n",
      "2025-05-23 14:26:38,463 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2025-05-23 14:26:38,463 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12.445937584521094 word corpus (23.5%% of prior 53)', 'datetime': '2025-05-23T14:26:38.463166', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 14:26:38,463 : INFO : estimated required memory for 40 words and 100 dimensions: 52000 bytes\n",
      "2025-05-23 14:26:38,463 : INFO : resetting layer weights\n",
      "2025-05-23 14:26:38,463 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-23T14:26:38.463166', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-05-23 14:26:38,469 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 40 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2025-05-23T14:26:38.469788', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:26:38,473 : INFO : EPOCH 0: training on 53 raw words (15 effective words) took 0.0s, 13841 effective words/s\n",
      "2025-05-23 14:26:38,478 : INFO : EPOCH 1: training on 53 raw words (13 effective words) took 0.0s, 25773 effective words/s\n",
      "2025-05-23 14:26:38,478 : INFO : EPOCH 2: training on 53 raw words (14 effective words) took 0.0s, 44515 effective words/s\n",
      "2025-05-23 14:26:38,488 : INFO : EPOCH 3: training on 53 raw words (13 effective words) took 0.0s, 90909 effective words/s\n",
      "2025-05-23 14:26:38,494 : INFO : EPOCH 4: training on 53 raw words (11 effective words) took 0.0s, 11896 effective words/s\n",
      "2025-05-23 14:26:38,494 : INFO : Word2Vec lifecycle event {'msg': 'training on 265 raw words (66 effective words) took 0.0s, 2700 effective words/s', 'datetime': '2025-05-23T14:26:38.494123', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 14:26:38,494 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=40, vector_size=100, alpha=0.025>', 'datetime': '2025-05-23T14:26:38.494123', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-23 14:26:38,494 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_trained_2.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-23T14:26:38.494123', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-05-23 14:26:38,494 : INFO : not storing attribute cum_table\n",
      "2025-05-23 14:26:38,494 : INFO : saved word2vec_trained_2.model\n",
      "2025-05-23 14:26:38,502 : INFO : loading Word2Vec object from word2vec_trained_2.model\n",
      "2025-05-23 14:26:38,518 : INFO : loading wv recursively from word2vec_trained_2.model.wv.* with mmap=None\n",
      "2025-05-23 14:26:38,518 : INFO : setting ignored attribute cum_table to None\n",
      "2025-05-23 14:26:38,521 : INFO : Word2Vec lifecycle event {'fname': 'word2vec_trained_2.model', 'datetime': '2025-05-23T14:26:38.521733', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "docs_2 = [\"나는 NLP를 공부하고 있다.\",\n",
    "\"자연어 처리는 인공지능의 중요한 분야이다.\",\n",
    "\"딥러닝과 머신러닝을 활용하여 NLP 연구를 진행한다.\",\n",
    "\"컴퓨터는 인간의 언어를 이해할 수 있을까?\",\n",
    "\"한국어 NLP는 영어보다 어려운 점이 많다.\",\n",
    "]\n",
    "\n",
    "data_2 = [tokens(d) for d in tqdm(docs_2, desc=\"Tokenizing_second\")]\n",
    "\n",
    "w2v_model = Word2Vec(tqdm(data_2, desc=\"Training Word2Vec_second\"), vector_size=100, window=3, sg=1, min_count=1)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "w2v_model.save('word2vec_trained_2.model')\n",
    "\n",
    "# 모델 다시 불러오기\n",
    "model_2 = Word2Vec.load('word2vec_trained_2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "476b43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력한 문장: 자연어\n",
      "[('./Punctuation', 0.21917827427387238), ('를/Josa', 0.1748206466436386), ('있다/Adjective', 0.1646600067615509), ('늘다/Verb', 0.14230895042419434), ('러닝/Noun', 0.10904104262590408)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text = input(\"Enter a sentence: \")\n",
    "    print(\"입력한 문장:\", text)\n",
    "    print(model_2.wv.most_similar(positive=tokenize(text), topn=5))\n",
    "except KeyError:\n",
    "    print(\"해당 단어는 학습되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13244fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('박보영/Noun', 0.6900759339332581), ('하지원/Noun', 0.6764721274375916), ('테론/Noun', 0.6622906923294067), ('곽지민/Noun', 0.6491292715072632), ('신세경/Noun', 0.6464618444442749)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=tokenize(u'왕 여자'),negative=tokenize(u'남자'), topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
