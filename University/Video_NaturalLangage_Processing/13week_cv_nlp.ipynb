{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyQOmlgQ2GYK",
        "outputId": "7210b7b6-7a01-49a5-d85d-c184177b1374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC5lcMSjwzFi",
        "outputId": "2e0f61e9-9190-42ec-c125-509a8e04b6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# í˜„ì¬ TensorFlowê°€ ì¸ì‹í•œ GPU ì¥ì¹˜ ëª©ë¡ ì¶œë ¥\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "l1Bc2wG82VyC",
        "outputId": "4f61123d-d773-4f85-da8d-86d8cc394268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...ì „ì²˜ë¦¬í•˜ê¸°...\n",
            "x_train shape: (150000, 100)\n",
            "x_test shape: (50000, 100)\n",
            "...ëª¨ë¸ë§Œë“¤ê¸°...\n",
            "...í•™ìŠµ...\n",
            "Epoch 1/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 56ms/step - accuracy: 0.7393 - loss: 0.4882 - val_accuracy: 0.7846 - val_loss: 0.4211\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 55ms/step - accuracy: 0.7949 - loss: 0.3985 - val_accuracy: 0.7864 - val_loss: 0.4172\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 54ms/step - accuracy: 0.8079 - loss: 0.3738 - val_accuracy: 0.7837 - val_loss: 0.4211\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 55ms/step - accuracy: 0.8162 - loss: 0.3574 - val_accuracy: 0.7828 - val_loss: 0.4250\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 55ms/step - accuracy: 0.8218 - loss: 0.3425 - val_accuracy: 0.7813 - val_loss: 0.4458\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 15ms/step - accuracy: 0.7844 - loss: 0.4379\n",
            "Test score: 0.44582897424697876\n",
            "Test accuracy: 0.7813199758529663\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU ë©”ëª¨ë¦¬ ì„¤ì • (ë©”ëª¨ë¦¬ ì¦ê°€ ë°©ì§€)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# ë°ì´í„° ì½ê¸°\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        result = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        return result[1:]  # header ì œì™¸\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "#Colab í™˜ê²½ì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "#train_tmp = read_data('/content/drive/MyDrive/ratings_train.txt')\n",
        "#test_tmp = read_data('/content/drive/MyDrive/ratings_test.txt')\n",
        "\n",
        "# ë¡œì»¬ í™˜ê²½ì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "train_tmp = read_data('./ratings_train.txt')\n",
        "test_tmp = read_data('./ratings_test.txt')\n",
        "\n",
        "def kor_movie(max_num_words=1000):\n",
        "    train_x, train_y = zip(*[(x[1], int(x[2])) for x in train_tmp])\n",
        "    test_x, test_y = zip(*[(x[1], int(x[2])) for x in test_tmp])\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=max_num_words)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    token_train_x = tokenizer.texts_to_sequences(train_x)\n",
        "    token_test_x = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    return (token_train_x, list(train_y)), (token_test_x, list(test_y)), tokenizer\n",
        "\n",
        "print('...ì „ì²˜ë¦¬í•˜ê¸°...')\n",
        "max_num_words = 5000\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test), tokenizer = kor_movie(max_num_words)\n",
        "\n",
        "# y ë°ì´í„° ë³€í™˜\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# ì…ë ¥ ë°ì´í„° íŒ¨ë”©\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "print('...ëª¨ë¸ë§Œë“¤ê¸°...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_num_words, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# ì»´íŒŒì¼\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('...í•™ìŠµ...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyxxPhgC2WG0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.save(\"sentiment_analysis_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizerê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# í›ˆë ¨í•œ Tokenizer ì €ì¥\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"Tokenizerê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
            "ë¦¬ë·°: ì´ ì˜í™” ì •ë§ ì¬ë°Œì–´ìš”!\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ì™„ì „ ìµœì•…ì´ì—ˆì–´. ì‹œê°„ ë‚­ë¹„.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ë°°ìš°ë“¤ ì—°ê¸° ìµœê³ ! ê°ë™ì ì´ì—ˆì–´ìš”.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´. ê¸°ëŒ€ ì´í•˜.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ì•¡ì…˜ ì¥ë©´ì´ ë©‹ì¡Œë‹¤. ëª°ì…ê° ìµœê³ !\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ë„ˆë¬´ ì§€ë£¨í•´ì„œ ì¤‘ê°„ì— ë‚˜ì™”ì–´ìš”.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: OSTê°€ ì¢‹ì•˜ì–´ìš”. ë¶„ìœ„ê¸°ê°€ ì™„ë²½í–ˆì–´.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ì´ëŸ° ì˜í™”ê°€ ì™œ ì¸ê¸°ì¸ì§€ ëª¨ë¥´ê² ë„¤.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ë§ˆì§€ë§‰ ë°˜ì „ ëŒ€ë°•! ì†Œë¦„ ë‹ì•˜ì–´.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n",
            "ë¦¬ë·°: ì—°ì¶œì´ ì–´ìƒ‰í•˜ê³  ëŒ€ì‚¬ê°€ ìœ ì¹˜í–ˆìŒ.\n",
            "ì˜ˆì¸¡ ê°ì„±: ê¸ì • ğŸ˜Š (í™•ë¥ : 0.5075)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model = tf.keras.models.load_model(\"sentiment_analysis_model.keras\")\n",
        "\n",
        "# Tokenizer ìƒˆë¡œ ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# ìƒˆë¡œìš´ ì˜í™” ë¦¬ë·° ë°ì´í„°\n",
        "new_reviews = [\n",
        "    \"ì´ ì˜í™” ì •ë§ ì¬ë°Œì–´ìš”!\",\n",
        "    \"ì™„ì „ ìµœì•…ì´ì—ˆì–´. ì‹œê°„ ë‚­ë¹„.\",\n",
        "    \"ë°°ìš°ë“¤ ì—°ê¸° ìµœê³ ! ê°ë™ì ì´ì—ˆì–´ìš”.\",\n",
        "    \"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´. ê¸°ëŒ€ ì´í•˜.\",\n",
        "    \"ì•¡ì…˜ ì¥ë©´ì´ ë©‹ì¡Œë‹¤. ëª°ì…ê° ìµœê³ !\",\n",
        "    \"ë„ˆë¬´ ì§€ë£¨í•´ì„œ ì¤‘ê°„ì— ë‚˜ì™”ì–´ìš”.\",\n",
        "    \"OSTê°€ ì¢‹ì•˜ì–´ìš”. ë¶„ìœ„ê¸°ê°€ ì™„ë²½í–ˆì–´.\",\n",
        "    \"ì´ëŸ° ì˜í™”ê°€ ì™œ ì¸ê¸°ì¸ì§€ ëª¨ë¥´ê² ë„¤.\",\n",
        "    \"ë§ˆì§€ë§‰ ë°˜ì „ ëŒ€ë°•! ì†Œë¦„ ë‹ì•˜ì–´.\",\n",
        "    \"ì—°ì¶œì´ ì–´ìƒ‰í•˜ê³  ëŒ€ì‚¬ê°€ ìœ ì¹˜í–ˆìŒ.\"\n",
        "]\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë¥¼ Tokenizerë¡œ ë³€í™˜\n",
        "tokenizer.fit_on_texts(new_reviews)  # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ Tokenizer í•™ìŠµ\n",
        "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
        "\n",
        "# íŒ¨ë”© ì¶”ê°€ (ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ max_lenê³¼ ë™ì¼í•´ì•¼ í•¨)\n",
        "max_len = 100  # ê¸°ì¡´ ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ max_len ê°’\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "# ê°ì„± ì˜ˆì¸¡ ì‹¤í–‰\n",
        "predictions = model.predict(new_padded)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    sentiment = \"ê¸ì • ğŸ˜Š\" if pred > 0.5 else \"ë¶€ì • ğŸ˜\"\n",
        "    print(f\"ë¦¬ë·°: {review}\\nì˜ˆì¸¡ ê°ì„±: {sentiment} (í™•ë¥ : {pred[0]:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150000\n",
            "3\n",
            "50000\n",
            "3\n",
            "2159921\n",
            "===========================================================\n",
            "SVM ë¶„ë¥˜ê¸° ì‹¤í–‰\n",
            "0.74594\n",
            "===========================================================\n"
          ]
        }
      ],
      "source": [
        "#-*- coding: utf-8 -*-\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "\n",
        "# SVM ëª¨ë¸ êµ¬í˜„ì„ ìœ„í•œ API (1)\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "\n",
        "# íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:] #header ì œì™¸\n",
        "    return data\n",
        "    \n",
        "train_data = read_data('./ratings_train.txt') \n",
        "test_data = read_data('./ratings_test.txt') #id, text, sentiment\n",
        "\n",
        "\n",
        "# Data ê°œìˆ˜ í™•ì¸\n",
        "print(len(train_data)) # train_data : 150,000\n",
        "print(len(train_data[0])) #ë³€ìˆ˜ : id, text, sentiment\n",
        "\n",
        "print(len(test_data)) # test_data : 50,000\n",
        "print(len(test_data[0])) #ë³€ìˆ˜ : id, text, sentiment\n",
        "\n",
        "# KoNLPy ì˜ íŠ¸ìœ„í„° í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•œ í† í°í™” (2)\n",
        "pos_tagger = Okt()\n",
        "\n",
        "def tokenize(doc):\n",
        "    return['/'.join(t) for t in pos_tagger.pos(doc,norm=True,stem=True)]\n",
        "\n",
        "train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
        "test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
        "\n",
        "# ë°ì´í„° ìš”ì•½ (3)\n",
        "tokens = [t for d in train_docs for t in d[0]] #ê° ë¬¸ì¥ë³„ë¡œ tokenizeë˜ìˆëŠ” ê±¸ unlistí™”\n",
        "print(len(tokens))\n",
        "text = nltk.Text(tokens, name='NMSC') \n",
        "\n",
        "# classification ìƒìœ„ 200ê°œ tokenë§Œ ë³€ìˆ˜ë¡œ ì‚¬ìš© (4)\n",
        "selected_words = [f[0] for f in text.vocab().most_common(200)]\n",
        "def term_exists(doc):\n",
        "    return {'exists({})'.format(word): \n",
        "    \t\t(word in set(doc)) for word in selected_words}\n",
        "\n",
        "train_docs = train_docs[:50000] #trainingì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ 50000ê°œë§Œ ì‚¬ìš©\n",
        "    \n",
        "train_xy = [(term_exists(d),c) for d,c in train_docs] #tf matrixë‘ ë¹„ìŠ·í•œ ê°œë…\n",
        "test_xy =  [(term_exists(d),c) for d,c in test_docs]\n",
        "\n",
        "# ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  í›ˆë ¨ ë° í‰ê°€ (5)\n",
        "print(\"===========================================================\")\n",
        "print(\"SVM ë¶„ë¥˜ê¸° ì‹¤í–‰\")\n",
        "classif = SklearnClassifier(LinearSVC())\n",
        "classifier_svm = classif.train(train_xy) \n",
        "print(nltk.classify.accuracy(classifier_svm, test_xy))\n",
        "print(\"===========================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# í•™ìŠµëœ SVM ëª¨ë¸ ì €ì¥\n",
        "with open(\"svm_classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(classifier_svm, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¦¬ë·°: ì´ ì˜í™” ì •ë§ ì¬ë°Œì–´ìš”!\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ì™„ì „ ìµœì•…ì´ì—ˆì–´. ì‹œê°„ ë‚­ë¹„.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ë°°ìš°ë“¤ ì—°ê¸° ìµœê³ ! ê°ë™ì ì´ì—ˆì–´ìš”.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´. ê¸°ëŒ€ ì´í•˜.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ì•¡ì…˜ ì¥ë©´ì´ ë©‹ì¡Œë‹¤. ëª°ì…ê° ìµœê³ !\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ë„ˆë¬´ ì§€ë£¨í•´ì„œ ì¤‘ê°„ì— ë‚˜ì™”ì–´ìš”.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: OSTê°€ ì¢‹ì•˜ì–´ìš”. ë¶„ìœ„ê¸°ê°€ ì™„ë²½í–ˆì–´.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ì´ëŸ° ì˜í™”ê°€ ì™œ ì¸ê¸°ì¸ì§€ ëª¨ë¥´ê² ë„¤.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ë§ˆì§€ë§‰ ë°˜ì „ ëŒ€ë°•! ì†Œë¦„ ë‹ì•˜ì–´.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n",
            "ë¦¬ë·°: ì—°ì¶œì´ ì–´ìƒ‰í•˜ê³  ëŒ€ì‚¬ê°€ ìœ ì¹˜í–ˆìŒ.\n",
            "ì˜ˆì¸¡ ê°ì„±: ë¶€ì • ğŸ˜\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# ì €ì¥ëœ SVM ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "with open(\"svm_classifier.pkl\", \"rb\") as f:\n",
        "    loaded_classifier = pickle.load(f)\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
        "pos_tagger = Okt()\n",
        "\n",
        "def tokenize(doc):\n",
        "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
        "\n",
        "# ê°ì„± ë¶„ì„ì„ ìœ„í•œ ì˜í™” ë¦¬ë·° ë°ì´í„°\n",
        "new_reviews = [\n",
        "    \"ì´ ì˜í™” ì •ë§ ì¬ë°Œì–´ìš”!\",\n",
        "    \"ì™„ì „ ìµœì•…ì´ì—ˆì–´. ì‹œê°„ ë‚­ë¹„.\",\n",
        "    \"ë°°ìš°ë“¤ ì—°ê¸° ìµœê³ ! ê°ë™ì ì´ì—ˆì–´ìš”.\",\n",
        "    \"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´. ê¸°ëŒ€ ì´í•˜.\",\n",
        "    \"ì•¡ì…˜ ì¥ë©´ì´ ë©‹ì¡Œë‹¤. ëª°ì…ê° ìµœê³ !\",\n",
        "    \"ë„ˆë¬´ ì§€ë£¨í•´ì„œ ì¤‘ê°„ì— ë‚˜ì™”ì–´ìš”.\",\n",
        "    \"OSTê°€ ì¢‹ì•˜ì–´ìš”. ë¶„ìœ„ê¸°ê°€ ì™„ë²½í–ˆì–´.\",\n",
        "    \"ì´ëŸ° ì˜í™”ê°€ ì™œ ì¸ê¸°ì¸ì§€ ëª¨ë¥´ê² ë„¤.\",\n",
        "    \"ë§ˆì§€ë§‰ ë°˜ì „ ëŒ€ë°•! ì†Œë¦„ ë‹ì•˜ì–´.\",\n",
        "    \"ì—°ì¶œì´ ì–´ìƒ‰í•˜ê³  ëŒ€ì‚¬ê°€ ìœ ì¹˜í–ˆìŒ.\"\n",
        "]\n",
        "# ì €ì¥ëœ ë‹¨ì–´ ì‚¬ì „ ë¡œë“œ\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    selected_words = pickle.load(f)\n",
        "\n",
        "selected_words = list(tokenizer.word_index.keys())\n",
        "\n",
        "def term_exists(doc):\n",
        "    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}\n",
        "\n",
        "# ê° ë¦¬ë·°ì— ëŒ€í•œ ê°ì„± ë¶„ì„ ìˆ˜í–‰\n",
        "for review in new_reviews:\n",
        "    tokens = tokenize(review)\n",
        "    features = term_exists(tokens)\n",
        "    prediction = loaded_classifier.classify(features)\n",
        "    sentiment = \"ê¸ì • ğŸ˜Š\" if prediction == '1' else \"ë¶€ì • ğŸ˜\"\n",
        "    print(f\"ë¦¬ë·°: {review}\\nì˜ˆì¸¡ ê°ì„±: {sentiment}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
