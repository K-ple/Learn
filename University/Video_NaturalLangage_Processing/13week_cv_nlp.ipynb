{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyQOmlgQ2GYK",
        "outputId": "7210b7b6-7a01-49a5-d85d-c184177b1374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC5lcMSjwzFi",
        "outputId": "2e0f61e9-9190-42ec-c125-509a8e04b6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 현재 TensorFlow가 인식한 GPU 장치 목록 출력\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "l1Bc2wG82VyC",
        "outputId": "4f61123d-d773-4f85-da8d-86d8cc394268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...전처리하기...\n",
            "x_train shape: (150000, 100)\n",
            "x_test shape: (50000, 100)\n",
            "...모델만들기...\n",
            "...학습...\n",
            "Epoch 1/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 56ms/step - accuracy: 0.7393 - loss: 0.4882 - val_accuracy: 0.7846 - val_loss: 0.4211\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 55ms/step - accuracy: 0.7949 - loss: 0.3985 - val_accuracy: 0.7864 - val_loss: 0.4172\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 54ms/step - accuracy: 0.8079 - loss: 0.3738 - val_accuracy: 0.7837 - val_loss: 0.4211\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 55ms/step - accuracy: 0.8162 - loss: 0.3574 - val_accuracy: 0.7828 - val_loss: 0.4250\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 55ms/step - accuracy: 0.8218 - loss: 0.3425 - val_accuracy: 0.7813 - val_loss: 0.4458\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 15ms/step - accuracy: 0.7844 - loss: 0.4379\n",
            "Test score: 0.44582897424697876\n",
            "Test accuracy: 0.7813199758529663\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU 메모리 설정 (메모리 증가 방지)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# 데이터 읽기\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        result = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        return result[1:]  # header 제외\n",
        "\n",
        "# 파일 경로\n",
        "#Colab 환경에서 파일 경로를 지정합니다.\n",
        "#train_tmp = read_data('/content/drive/MyDrive/ratings_train.txt')\n",
        "#test_tmp = read_data('/content/drive/MyDrive/ratings_test.txt')\n",
        "\n",
        "# 로컬 환경에서 파일 경로를 지정합니다.\n",
        "train_tmp = read_data('./ratings_train.txt')\n",
        "test_tmp = read_data('./ratings_test.txt')\n",
        "\n",
        "def kor_movie(max_num_words=1000):\n",
        "    train_x, train_y = zip(*[(x[1], int(x[2])) for x in train_tmp])\n",
        "    test_x, test_y = zip(*[(x[1], int(x[2])) for x in test_tmp])\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=max_num_words)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    token_train_x = tokenizer.texts_to_sequences(train_x)\n",
        "    token_test_x = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    return (token_train_x, list(train_y)), (token_test_x, list(test_y)), tokenizer\n",
        "\n",
        "print('...전처리하기...')\n",
        "max_num_words = 5000\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test), tokenizer = kor_movie(max_num_words)\n",
        "\n",
        "# y 데이터 변환\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# 입력 데이터 패딩\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "\n",
        "# 모델 생성\n",
        "print('...모델만들기...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_num_words, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('...학습...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyxxPhgC2WG0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.save(\"sentiment_analysis_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer가 성공적으로 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# 훈련한 Tokenizer 저장\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"Tokenizer가 성공적으로 저장되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
            "리뷰: 이 영화 정말 재밌어요!\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 완전 최악이었어. 시간 낭비.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 배우들 연기 최고! 감동적이었어요.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 스토리가 너무 뻔해. 기대 이하.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 액션 장면이 멋졌다. 몰입감 최고!\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 너무 지루해서 중간에 나왔어요.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: OST가 좋았어요. 분위기가 완벽했어.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 이런 영화가 왜 인기인지 모르겠네.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 마지막 반전 대박! 소름 돋았어.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n",
            "리뷰: 연출이 어색하고 대사가 유치했음.\n",
            "예측 감성: 긍정 😊 (확률: 0.5075)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 저장된 모델 불러오기\n",
        "model = tf.keras.models.load_model(\"sentiment_analysis_model.keras\")\n",
        "\n",
        "# Tokenizer 새로 생성\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# 새로운 영화 리뷰 데이터\n",
        "new_reviews = [\n",
        "    \"이 영화 정말 재밌어요!\",\n",
        "    \"완전 최악이었어. 시간 낭비.\",\n",
        "    \"배우들 연기 최고! 감동적이었어요.\",\n",
        "    \"스토리가 너무 뻔해. 기대 이하.\",\n",
        "    \"액션 장면이 멋졌다. 몰입감 최고!\",\n",
        "    \"너무 지루해서 중간에 나왔어요.\",\n",
        "    \"OST가 좋았어요. 분위기가 완벽했어.\",\n",
        "    \"이런 영화가 왜 인기인지 모르겠네.\",\n",
        "    \"마지막 반전 대박! 소름 돋았어.\",\n",
        "    \"연출이 어색하고 대사가 유치했음.\"\n",
        "]\n",
        "\n",
        "# 텍스트를 Tokenizer로 변환\n",
        "tokenizer.fit_on_texts(new_reviews)  # 새로운 텍스트에 대해 Tokenizer 학습\n",
        "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
        "\n",
        "# 패딩 추가 (모델 학습 시 사용한 max_len과 동일해야 함)\n",
        "max_len = 100  # 기존 모델 학습 시 사용한 max_len 값\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "# 감성 예측 실행\n",
        "predictions = model.predict(new_padded)\n",
        "\n",
        "# 결과 출력\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    sentiment = \"긍정 😊\" if pred > 0.5 else \"부정 😞\"\n",
        "    print(f\"리뷰: {review}\\n예측 감성: {sentiment} (확률: {pred[0]:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150000\n",
            "3\n",
            "50000\n",
            "3\n",
            "2159921\n",
            "===========================================================\n",
            "SVM 분류기 실행\n",
            "0.74594\n",
            "===========================================================\n"
          ]
        }
      ],
      "source": [
        "#-*- coding: utf-8 -*-\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "\n",
        "# SVM 모델 구현을 위한 API (1)\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:] #header 제외\n",
        "    return data\n",
        "    \n",
        "train_data = read_data('./ratings_train.txt') \n",
        "test_data = read_data('./ratings_test.txt') #id, text, sentiment\n",
        "\n",
        "\n",
        "# Data 개수 확인\n",
        "print(len(train_data)) # train_data : 150,000\n",
        "print(len(train_data[0])) #변수 : id, text, sentiment\n",
        "\n",
        "print(len(test_data)) # test_data : 50,000\n",
        "print(len(test_data[0])) #변수 : id, text, sentiment\n",
        "\n",
        "# KoNLPy 의 트위터 형태소 분석기를 통한 토큰화 (2)\n",
        "pos_tagger = Okt()\n",
        "\n",
        "def tokenize(doc):\n",
        "    return['/'.join(t) for t in pos_tagger.pos(doc,norm=True,stem=True)]\n",
        "\n",
        "train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
        "test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
        "\n",
        "# 데이터 요약 (3)\n",
        "tokens = [t for d in train_docs for t in d[0]] #각 문장별로 tokenize되있는 걸 unlist화\n",
        "print(len(tokens))\n",
        "text = nltk.Text(tokens, name='NMSC') \n",
        "\n",
        "# classification 상위 200개 token만 변수로 사용 (4)\n",
        "selected_words = [f[0] for f in text.vocab().most_common(200)]\n",
        "def term_exists(doc):\n",
        "    return {'exists({})'.format(word): \n",
        "    \t\t(word in set(doc)) for word in selected_words}\n",
        "\n",
        "train_docs = train_docs[:50000] #training시간 단축을 위해 50000개만 사용\n",
        "    \n",
        "train_xy = [(term_exists(d),c) for d,c in train_docs] #tf matrix랑 비슷한 개념\n",
        "test_xy =  [(term_exists(d),c) for d,c in test_docs]\n",
        "\n",
        "# 서포트 벡터 머신 훈련 및 평가 (5)\n",
        "print(\"===========================================================\")\n",
        "print(\"SVM 분류기 실행\")\n",
        "classif = SklearnClassifier(LinearSVC())\n",
        "classifier_svm = classif.train(train_xy) \n",
        "print(nltk.classify.accuracy(classifier_svm, test_xy))\n",
        "print(\"===========================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# 학습된 SVM 모델 저장\n",
        "with open(\"svm_classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(classifier_svm, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "리뷰: 이 영화 정말 재밌어요!\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 완전 최악이었어. 시간 낭비.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 배우들 연기 최고! 감동적이었어요.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 스토리가 너무 뻔해. 기대 이하.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 액션 장면이 멋졌다. 몰입감 최고!\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 너무 지루해서 중간에 나왔어요.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: OST가 좋았어요. 분위기가 완벽했어.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 이런 영화가 왜 인기인지 모르겠네.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 마지막 반전 대박! 소름 돋았어.\n",
            "예측 감성: 부정 😞\n",
            "\n",
            "리뷰: 연출이 어색하고 대사가 유치했음.\n",
            "예측 감성: 부정 😞\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 저장된 SVM 모델 불러오기\n",
        "with open(\"svm_classifier.pkl\", \"rb\") as f:\n",
        "    loaded_classifier = pickle.load(f)\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "pos_tagger = Okt()\n",
        "\n",
        "def tokenize(doc):\n",
        "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
        "\n",
        "# 감성 분석을 위한 영화 리뷰 데이터\n",
        "new_reviews = [\n",
        "    \"이 영화 정말 재밌어요!\",\n",
        "    \"완전 최악이었어. 시간 낭비.\",\n",
        "    \"배우들 연기 최고! 감동적이었어요.\",\n",
        "    \"스토리가 너무 뻔해. 기대 이하.\",\n",
        "    \"액션 장면이 멋졌다. 몰입감 최고!\",\n",
        "    \"너무 지루해서 중간에 나왔어요.\",\n",
        "    \"OST가 좋았어요. 분위기가 완벽했어.\",\n",
        "    \"이런 영화가 왜 인기인지 모르겠네.\",\n",
        "    \"마지막 반전 대박! 소름 돋았어.\",\n",
        "    \"연출이 어색하고 대사가 유치했음.\"\n",
        "]\n",
        "# 저장된 단어 사전 로드\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    selected_words = pickle.load(f)\n",
        "\n",
        "selected_words = list(tokenizer.word_index.keys())\n",
        "\n",
        "def term_exists(doc):\n",
        "    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}\n",
        "\n",
        "# 각 리뷰에 대한 감성 분석 수행\n",
        "for review in new_reviews:\n",
        "    tokens = tokenize(review)\n",
        "    features = term_exists(tokens)\n",
        "    prediction = loaded_classifier.classify(features)\n",
        "    sentiment = \"긍정 😊\" if prediction == '1' else \"부정 😞\"\n",
        "    print(f\"리뷰: {review}\\n예측 감성: {sentiment}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
